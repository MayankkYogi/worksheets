2:

The coefficient of determination, R2, is a statistical measure that shows the proportion of variation explained by the estimated regression line. Variation refers to the sum of the squared differences between the values of Y and the mean value of Y, expressed mathematically as
R2 always takes on a value between 0 and 1. The closer R2 is to 1, the better the estimated regression equation fits or explains the relationship between X and Y.

Residual sum of squares (RSS): This expression is also known as unexplained variation and is the portion of total variation that measures discrepancies (errors) between the actual values of Y and those estimated by the regression equation.
The smaller the value of RSS relative to ESS, the better the regression line fits or explains the relationship between the dependent and independent variable.



4:

Gini index or Gini impurity measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen. But what is actually meant by ‘impurity’? If all the elements belong to a single class, then it can be called pure. The degree of Gini index varies between 0 and 1, where 0 denotes that all elements belong to a certain class or if there exists only one class, and 1 denotes that the elements are randomly distributed across various classes. A Gini Index of 0.5 denotes equally distributed elements into some classes.Gini Impurity is a measurement of the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set.Gini impurity is lower bounded by 0, with 0 occurring if the data set contains only one class.


5:

In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set. Thus it ends up with branches with strict rules of sparse data. Thus this effects the accuracy when predicting samples that are not part of the training set.
One of the methods used to address over-fitting in decision tree is called pruning which is done after the initial training is complete. In pruning, you trim off the branches of the tree, i.e., remove the decision nodes starting from the leaf node such that the overall accuracy is not disturbed. This is done by segregating the actual training set into two sets: training data set, D and validation data set, V. Prepare the decision tree using the segregated training data set, D. Then continue trimming the tree accordingly to optimize the accuracy of the validation data set, V.




6 :

Ensemble Techniques are consists of bagging and boosting algorithms which helps to boost the performance. These techniques consists of various algorithms and all these algo used decision tree internally.In this there are multiple weak learner model which combine or aggregate at the end and make the best model which provide high rate of accuracy and also perform well with the test data. These techniques also have low bias and low variance.
Boosting Techniques are:
1 - Bagging
2 - Boosting

Bagging Techniques include Random Forest which used decision tree internally
Boosting Techniques  include XG Boost,AdaBoost etc


7:

Bagging : 
- this is an type of ensemble techniques.
- this include random forest
- this uses decision tree internally 
- in this type multiple weak learn model predict the output and at the end side output havving max prob will be choose as output
- this also called as bootstrap aggregator

Boosting : 
- this is also a type of ensemble technique
- this include XG Boost, ada boost etc
- this model also used drcison tree inside
- here the weak learners are sequentially produced during the training phase. The performance of the model is improved by assigning a higher weightage to the previous,     incorrectly classified samples.


8:

Random forests technique involves sampling of the input data with replacement (bootstrap sampling). In this sampling, about one thrird of the data is not used for training and can be used to testing.These are called the out of bag samples. Error estimated on these out of bag samples is the out of bag error.


9 : 

K-Fold cross validation is used to make a genralized model,means when we use random state most if the time at every random state the accuracy and report are different, means if the data get change in future then the model will perform in different manner which is not a good practice, so to avoid this we should use a way where we can make a gernal model so there come cross validation techniques.
There are type of cross validation which are:
- K Fold
- Stratified K Fold
- time series

In K Fold, the data set is divided in two parts train and test, if we have 1000 record in data set and we set k value as 5 , so 1000/5 = 200, this 200 will be the size of test data, and this will be randomly taken from data set 5 times as we take k = 5, so at last the mean of all value is taken and provide as accuracy of model. Here we can mention cross validation value also.


10:

Hyper meter tunning in ML is used to improve the efficiency of model , there are various ways we can do hyperparameter tunning using GridSearchCV, RandomizesSearchCV and various more. In this we take out the best parameter of the model depending upon the data set, so example for one data set the model is provide moderate rate of accuracy , so by taking the right parametere value we can enhance the value of accuracy and improve the model.
There are multiple model which had their parameters like max_dept, min_sample_split, n_estimator etc

11:

If we have a large learning rate in Gradient Descent then at the time of back propagation the new weight which come will have similar or almost same value to old weight or also the new weight will come in negative number but the values should be increase and decrease from both side of the curve, which will increase the time to reach global minimum loss, so the good practice is to take the right learning rate value,When using high learning rates, it is possible to encounter a positive feedback loop in which large weights induce large gradients which then induce a large update to the weights. If these updates consistently increase the size of the weights, then weights rapidly moves away from the origin until numerical overflow occurs.

12:

bias:
Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.

variance:
Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.

If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.
This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.

To build a good model, we need to find a good balance between bias and variance such that it minimizes the total error.


13:

Regularization in machine learning is used to avoid the problem of overfitting problem , this regularization techniques include L1 ,L2 and drop out regularization which means Lasso and Ridge techniques.It helps to reduce the cost function , overfitting means the accuracy for train data is high but for test data the rate is very less so this is overfitting, this also happen if we have many parameter or features in the data set.
A regression model which uses L1 Regularisation technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression.
A regression model that uses L2 regularisation technique is called Ridge regression.

14:

Both methods use a set of weak learners. They try to boost these weak learners into a strong learner. I assume that the strong learner is additive by the weak learners.

Gradient boosting generates learners during the learning process. It build first learner to predict the values/labels of samples, and calculate the loss (the difference between the outcome of the first learner and the real value). It will build a second learner to predict the loss after the first step. The step continues to learn the third, forth… until certain threshold.

Adaboost requires users specify a set of weak learners (alternatively, it will randomly generate a set of weak learner before the real learning process). It will learn the weights of how to add these learners to be a strong learner. The weight of each learner is learned by whether it predicts a sample correctly or not. If a learner is mispredict a sample, the weight of the learner is reduced a bit. It will repeat such process until converge.
Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function(L).


15:
Yes we can use logistic regression for classification for non linear data because logistic regression change the values from 0 to 0.25 after derivative also we can use the logistic regression for binary class and multiclass classification (multiclass = 'ovr').
in this there is sigmoid function which is used and make value from 0 to 1 , and this can be used for classification type of problem.


